{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2036b15-cdef-4fe3-b640-88a719b4b6bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T02:59:29.687106Z",
     "iopub.status.busy": "2023-11-21T02:59:29.685988Z",
     "iopub.status.idle": "2023-11-21T02:59:32.232338Z",
     "shell.execute_reply": "2023-11-21T02:59:32.231201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\r\n",
      "=======================\r\n",
      "Status=Up/Down\r\n",
      "|/ State=Normal/Leaving/Joining/Moving\r\n",
      "--  Address     Load       Tokens  Owns (effective)  Host ID                               Rack \r\n",
      "UN  172.19.0.2  70.27 KiB  16      63.3%             e39cc177-675f-476a-919d-d9c612aa378c  rack1\r\n",
      "UN  172.19.0.4  70.26 KiB  16      59.8%             1a7fef71-6f85-4ad2-8272-7552e4e87b92  rack1\r\n",
      "UN  172.19.0.3  70.28 KiB  16      76.9%             ba439315-996e-4531-b1b8-c4f8981a603a  rack1\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a822d0e1-3e84-4fcd-a5a5-16dd7386b3e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T02:59:32.237701Z",
     "iopub.status.busy": "2023-11-21T02:59:32.237313Z",
     "iopub.status.idle": "2023-11-21T02:59:32.814708Z",
     "shell.execute_reply": "2023-11-21T02:59:32.813291Z"
    }
   },
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['p6-db-1', 'p6-db-2', 'p6-db-3'])\n",
    "cass = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59aecd18-ac92-4111-8958-38fba125e5a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T02:59:32.829085Z",
     "iopub.status.busy": "2023-11-21T02:59:32.828238Z",
     "iopub.status.idle": "2023-11-21T02:59:36.987017Z",
     "shell.execute_reply": "2023-11-21T02:59:36.985172Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f90d82a8940>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"DROP KEYSPACE IF EXISTS weather\")\n",
    "\n",
    "cass.execute(\"\"\"CREATE KEYSPACE weather WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '3'}\"\"\")\n",
    "\n",
    "cass.execute(\"use weather\")\n",
    "cass.execute(\"\"\"CREATE TYPE station_record ( tmin int, tmax int)\"\"\")\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "CREATE TABLE stations(id text, name text static, date date, record station_record, PRIMARY KEY (id, date)) WITH CLUSTERING ORDER BY (date ASC)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e75094a-1392-40a2-8cf8-fde9a0a93d75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T02:59:36.994875Z",
     "iopub.status.busy": "2023-11-21T02:59:36.993570Z",
     "iopub.status.idle": "2023-11-21T03:00:15.314057Z",
     "shell.execute_reply": "2023-11-21T03:00:15.312122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-133edacf-2b4b-426e-88bd-5e21b398ce2d;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.typesafe#config;1.4.1 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.12/3.4.0/spark-cassandra-connector_2.12-3.4.0.jar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector_2.12;3.4.0!spark-cassandra-connector_2.12.jar (178ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-driver_2.12/3.4.0/spark-cassandra-connector-driver_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0!spark-cassandra-connector-driver_2.12.jar (98ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-core-shaded/4.13.0/java-driver-core-shaded-4.13.0.jar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-core-shaded;4.13.0!java-driver-core-shaded.jar (275ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-mapper-runtime/4.13.0/java-driver-mapper-runtime-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-mapper-runtime;4.13.0!java-driver-mapper-runtime.jar(bundle) (38ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.10/commons-lang3-3.10.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.10!commons-lang3.jar (65ms)\n",
      "downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...\n",
      "\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (35ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.11/scala-reflect-2.12.11.jar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.11!scala-reflect.jar (143ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/native-protocol/1.5.0/native-protocol-1.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#native-protocol;1.5.0!native-protocol.jar(bundle) (38ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-shaded-guava/25.1-jre-graal-sub-1/java-driver-shaded-guava-25.1-jre-graal-sub-1.jar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1!java-driver-shaded-guava.jar (164ms)\n",
      "downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] com.typesafe#config;1.4.1!config.jar(bundle) (41ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.26/slf4j-api-1.7.26.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.26!slf4j-api.jar (32ms)\n",
      "downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/4.1.18/metrics-core-4.1.18.jar ...\n",
      "\t[SUCCESSFUL ] io.dropwizard.metrics#metrics-core;4.1.18!metrics-core.jar(bundle) (31ms)\n",
      "downloading https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar ...\n",
      "\t[SUCCESSFUL ] org.hdrhistogram#HdrHistogram;2.1.12!HdrHistogram.jar(bundle) (46ms)\n",
      "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.3!reactive-streams.jar (48ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (27ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar ...\n",
      "\t[SUCCESSFUL ] com.github.spotbugs#spotbugs-annotations;3.1.12!spotbugs-annotations.jar (36ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (27ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-query-builder/4.13.0/java-driver-query-builder-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-query-builder;4.13.0!java-driver-query-builder.jar(bundle) (33ms)\n",
      ":: resolution report :: resolve 5501ms :: artifacts dl 1393ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   18  |   18  |   0   ||   18  |   18  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-133edacf-2b4b-426e-88bd-5e21b398ce2d\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t18 artifacts copied, 0 already retrieved (18067kB/79ms)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/21 02:59:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "from datetime import datetime\n",
    "current_date = datetime.now().date()\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"p6\")\n",
    "         .config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.0')\n",
    "         .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "         .getOrCreate())\n",
    "\n",
    "stations_df = spark.read.text(\"/nb/ghcnd-stations.txt\")\n",
    "stations_processed = stations_df.select(\n",
    "    expr(\"substring(value, 1, 11)\").alias(\"ID\"),\n",
    "    expr(\"substring(value, 39, 2)\").alias(\"STATE\"),\n",
    "    expr(\"substring(value, 42, 30)\").alias(\"NAME\")\n",
    ")\n",
    "wi_stations = stations_processed.filter(stations_processed.STATE == \"WI\")\n",
    "wi_stations_list = wi_stations.collect()\n",
    "\n",
    "\n",
    "for station in wi_stations_list:\n",
    "    cass.execute(\"\"\"\n",
    "        INSERT INTO stations (id,name)\n",
    "        VALUES (%s, %s)\n",
    "    \"\"\",\n",
    "        (station.ID, station.NAME)\n",
    "                )\n",
    "val = cass.execute(\"\"\"SELECT COUNT(*) FROM stations\"\"\")\n",
    "describe_table_query = \"DESCRIBE TABLE stations\"\n",
    "table_description = cass.execute(describe_table_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "865c61e0-246b-4074-9d39-ecd9f53a36bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T03:00:15.322189Z",
     "iopub.status.busy": "2023-11-21T03:00:15.321150Z",
     "iopub.status.idle": "2023-11-21T03:00:15.353201Z",
     "shell.execute_reply": "2023-11-21T03:00:15.351729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE weather.stations (\n",
      "    id text,\n",
      "    date date,\n",
      "    name text static,\n",
      "    record station_record,\n",
      "    PRIMARY KEY (id, date)\n",
      ") WITH CLUSTERING ORDER BY (date ASC)\n",
      "    AND additional_write_policy = '99p'\n",
      "    AND bloom_filter_fp_chance = 0.01\n",
      "    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\n",
      "    AND cdc = false\n",
      "    AND comment = ''\n",
      "    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\n",
      "    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\n",
      "    AND memtable = 'default'\n",
      "    AND crc_check_chance = 1.0\n",
      "    AND default_time_to_live = 0\n",
      "    AND extensions = {}\n",
      "    AND gc_grace_seconds = 864000\n",
      "    AND max_index_interval = 2048\n",
      "    AND memtable_flush_period_in_ms = 0\n",
      "    AND min_index_interval = 128\n",
      "    AND read_repair = 'BLOCKING'\n",
      "    AND speculative_retry = '99p';\n"
     ]
    }
   ],
   "source": [
    "#q1\n",
    "print((cass.execute(\"describe table stations\")).one().create_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81dfc882-c237-4eb7-b3fd-ba9e781fc07c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T03:00:15.361236Z",
     "iopub.status.busy": "2023-11-21T03:00:15.360049Z",
     "iopub.status.idle": "2023-11-21T03:00:15.383666Z",
     "shell.execute_reply": "2023-11-21T03:00:15.382362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MADISON DANE CO RGNL AP       '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "query = \"SELECT name FROM stations WHERE id = 'USW00014837'\"\n",
    "result = cass.execute(query)\n",
    "name = result.one()[0]\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09a2ee9c-16ac-446c-96e0-5f71a31d7e12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T03:00:15.390988Z",
     "iopub.status.busy": "2023-11-21T03:00:15.389586Z",
     "iopub.status.idle": "2023-11-21T03:00:15.487981Z",
     "shell.execute_reply": "2023-11-21T03:00:15.485574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9014250178872933741"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "result = cass.execute(\"SELECT token(id) FROM stations WHERE id = 'USC00470273'\")\n",
    "station_token = result.one()[0]\n",
    "station_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad1bc1ae-0a22-40bc-a515-867739c02649",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T03:00:15.506459Z",
     "iopub.status.busy": "2023-11-21T03:00:15.503308Z",
     "iopub.status.idle": "2023-11-21T03:00:18.153560Z",
     "shell.execute_reply": "2023-11-21T03:00:18.152357Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8725289218351686139"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q4\n",
    "from subprocess import check_output\n",
    "\n",
    "node_out = check_output([\"nodetool\", \"ring\"]).decode(\"utf-8\")\n",
    "tokens = []\n",
    "\n",
    "for line in node_out.split(\"\\n\"):\n",
    "    if 'Normal' in line:\n",
    "        tok = int(line.split()[-1])  \n",
    "        tokens.append(tok)\n",
    "        \n",
    "tokens.sort()\n",
    "\n",
    "next_token = None\n",
    "\n",
    "for tok in tokens:\n",
    "    if tok > station_token:\n",
    "        next_token = tok\n",
    "        break\n",
    "\n",
    "if next_token is None and tokens:\n",
    "    next_token = tokens[0]\n",
    "\n",
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b34dad54-23a2-479b-a175-32311443fc43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T03:00:33.172601Z",
     "iopub.status.busy": "2023-11-21T03:00:33.172097Z",
     "iopub.status.idle": "2023-11-21T03:00:51.856297Z",
     "shell.execute_reply": "2023-11-21T03:00:51.854892Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/21 03:00:33 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "356"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5\n",
    "import grpc\n",
    "import station_pb2\n",
    "import station_pb2_grpc\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TemperatureRecords\").getOrCreate()\n",
    "\n",
    "df = spark.read.parquet(\"./records.parquet\")\n",
    "df_tmax = df.filter(df.element == 'TMAX') \\\n",
    "    .withColumn(\"date\", to_date(col(\"date\").cast(\"string\"), 'yyyyMMdd')) \\\n",
    "    .withColumnRenamed(\"value\", \"tmax\")\n",
    "\n",
    "df_tmin = df.filter(df.element == 'TMIN') \\\n",
    "    .withColumn(\"date\", to_date(col(\"date\").cast(\"string\"), 'yyyyMMdd')) \\\n",
    "    .withColumnRenamed(\"value\", \"tmin\")\n",
    "\n",
    "df_joined = df_tmax.join(df_tmin, ['station', 'date'])\n",
    "\n",
    "channel = grpc.insecure_channel('localhost:5440')\n",
    "stub = station_pb2_grpc.StationStub(channel)\n",
    "\n",
    "for row in df_joined.collect():\n",
    "    station_id = row[\"station\"]\n",
    "    date = row[\"date\"].strftime(\"%Y-%m-%d\") \n",
    "    tmax = int(row[\"tmax\"])\n",
    "    tmin = int(row[\"tmin\"])\n",
    "    request = station_pb2.RecordTempsRequest(\n",
    "        station=station_id,\n",
    "        date=date,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax\n",
    "    )\n",
    "    response = stub.RecordTemps(request)\n",
    "\n",
    "response = stub.StationMax(station_pb2.StationMaxRequest(station=\"USW00014837\"))\n",
    "\n",
    "if not response.error:\n",
    "    max_temp = response.tmax\n",
    "else:\n",
    "    max_temp = \"Error: \" + response.error\n",
    "\n",
    "max_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cf729c6-88e0-4621-8f34-217e48157995",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T03:00:51.862897Z",
     "iopub.status.busy": "2023-11-21T03:00:51.861826Z",
     "iopub.status.idle": "2023-11-21T03:00:56.259465Z",
     "shell.execute_reply": "2023-11-21T03:00:56.258365Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='stations', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q6\n",
    "stationsDF = spark.read.format(\"org.apache.spark.sql.cassandra\").option(\"spark.cassandra.connection.host\", \"p6-db-1,p6-db-2,p6-db-3\").option(\"keyspace\", \"weather\").option(\"table\", \"stations\").load()\n",
    "stationsDF.createOrReplaceTempView(\"stations\")\n",
    "tables = spark.catalog.listTables()\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b71e457-2bf9-40d6-bf4c-fce27eff0567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T03:00:56.265759Z",
     "iopub.status.busy": "2023-11-21T03:00:56.264840Z",
     "iopub.status.idle": "2023-11-21T03:00:58.600758Z",
     "shell.execute_reply": "2023-11-21T03:00:58.599608Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USW00014837': 105.62739726027397,\n",
       " 'USW00014898': 102.93698630136986,\n",
       " 'USW00014839': 89.6986301369863,\n",
       " 'USR0000WDDG': 102.06849315068493}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q7\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col\n",
    "\n",
    "stations_of_interest = ['USW00014839', 'USR0000WDDG', 'USW00014837', 'USW00014898']\n",
    "df_filtered = df_joined.filter(col('station').isin(stations_of_interest))\n",
    "\n",
    "df_with_diff = df_filtered.withColumn('temp_diff', col('tmax') - col('tmin'))\n",
    "\n",
    "avg_diff_by_station = df_with_diff.groupBy('station').agg(avg('temp_diff').alias('avg_temp_diff'))\n",
    "\n",
    "avg_temp_diff_dict = {row['station']: row['avg_temp_diff'] for row in avg_diff_by_station.collect()}\n",
    "\n",
    "avg_temp_diff_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24a679b1-5336-4da9-acfa-cf7ee70d7f66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T03:01:35.642600Z",
     "iopub.status.busy": "2023-11-21T03:01:35.641141Z",
     "iopub.status.idle": "2023-11-21T03:01:37.729226Z",
     "shell.execute_reply": "2023-11-21T03:01:37.727258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\r\n",
      "=======================\r\n",
      "Status=Up/Down\r\n",
      "|/ State=Normal/Leaving/Joining/Moving\r\n",
      "--  Address     Load       Tokens  Owns (effective)  Host ID                               Rack \r\n",
      "UN  172.19.0.2  87.75 KiB  16      100.0%            e39cc177-675f-476a-919d-d9c612aa378c  rack1\r\n",
      "UN  172.19.0.4  87.75 KiB  16      100.0%            1a7fef71-6f85-4ad2-8272-7552e4e87b92  rack1\r\n",
      "DN  172.19.0.3  87.76 KiB  16      100.0%            ba439315-996e-4531-b1b8-c4f8981a603a  rack1\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#q8\n",
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f81718e-7c1f-4243-80c0-3a0ef47c1190",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T03:01:37.734992Z",
     "iopub.status.busy": "2023-11-21T03:01:37.734617Z",
     "iopub.status.idle": "2023-11-21T03:01:37.795157Z",
     "shell.execute_reply": "2023-11-21T03:01:37.792923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error field content: need 3 replicas, but only have 2\n"
     ]
    }
   ],
   "source": [
    "#q9\n",
    "channel = grpc.insecure_channel('localhost:5440')\n",
    "stub = station_pb2_grpc.StationStub(channel)\n",
    "\n",
    "request = station_pb2.StationMaxRequest(station=\"USR0000WDDG\")\n",
    "\n",
    "try:\n",
    "    response = stub.StationMax(request)\n",
    "    error_content = response.error \n",
    "except grpc.RpcError as e:\n",
    "    error_content = e.details()\n",
    "print(f\"Error field content: {error_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "052a313e-86ce-4802-b26e-4a56494cb6f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T03:01:37.804352Z",
     "iopub.status.busy": "2023-11-21T03:01:37.801702Z",
     "iopub.status.idle": "2023-11-21T03:01:37.835268Z",
     "shell.execute_reply": "2023-11-21T03:01:37.832134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: \n"
     ]
    }
   ],
   "source": [
    "#q10\n",
    "\n",
    "stub = station_pb2_grpc.StationStub(channel)\n",
    "\n",
    "request = station_pb2.RecordTempsRequest(station='blahblahblah',date='2023-04-01',tmin=15,tmax=25)\n",
    "\n",
    "try:\n",
    "    response = stub.RecordTemps(request)\n",
    "    error_message = response.error\n",
    "except grpc.RpcError as rpc_error:\n",
    "    error_message = rpc_error.details()\n",
    "\n",
    "print(\"error: \"+ error_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
